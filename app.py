"""
í“¨ì³ì‹œìŠ¤í…œ íšŒì‚¬ì†Œê°œ ì±—ë´‡
Streamlit ê¸°ë°˜ RAG ì±—ë´‡ (2025 ìµœì‹  ê¸°ìˆ  ìŠ¤íƒ)
ChromaDB + BGE-M3 + FlashRank + Ollama
"""

import streamlit as st
from datetime import datetime
from pathlib import Path
import logging
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from utils.rag_pipeline import RAGPipeline

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í“¨ì³ì‹œìŠ¤í…œ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)


@st.cache_resource
def initialize_rag_pipeline(model_name: str, temperature: float, use_reranking: bool):
    """RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ìºì‹±)"""
    try:
        with st.spinner("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            # RAG íŒŒì´í”„ë¼ì¸ ìƒì„±
            pipeline = RAGPipeline(
                model_name=model_name,
                temperature=temperature,
                use_reranking=use_reranking
            )

            # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
            vectorstore_path = project_root / "data" / "vectorstore"

            if not vectorstore_path.exists():
                st.error("ë²¡í„° ìŠ¤í† ì–´ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `python scripts/create_vectorstore.py`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
                return None

            pipeline.load_vectorstore(str(vectorstore_path))

            # QA ì²´ì¸ ìƒì„±
            pipeline.create_qa_chain()

            logger.info("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            return pipeline

    except Exception as e:
        st.error(f"RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error(f"RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        return None


def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""

    # ì œëª©
    st.title("ğŸ¤– í“¨ì³ì‹œìŠ¤í…œ íšŒì‚¬ì†Œê°œ ì±—ë´‡")
    st.markdown("í“¨ì³ì‹œìŠ¤í…œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš” (RAG + Ollama)")

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        st.markdown("---")

        # ëª¨ë¸ ì„ íƒ
        model_option = st.selectbox(
            "LLM ëª¨ë¸",
            ["llama3.1:8b", "llama3.2:3b", "mistral:7b", "gemma:7b"],
            help="Ollamaì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
        )

        # ì˜¨ë„ ì„¤ì •
        temperature = st.slider(
            "Temperature",
            0.0, 1.0, 0.7, 0.1,
            help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì "
        )

        # Reranking ì˜µì…˜
        use_reranking = st.checkbox(
            "Reranking ì‚¬ìš©",
            value=True,
            help="FlashRankë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”"
        )

        st.markdown("---")

        # ì‹œìŠ¤í…œ ì •ë³´
        st.info("ğŸ’¡ **ê¸°ìˆ  ìŠ¤íƒ**\n"
                "- Vector DB: ChromaDB\n"
                "- Embeddings: BGE-M3\n"
                "- Reranking: FlashRank\n"
                "- LLM: Ollama")

        st.warning("âš ï¸ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”\n"
                   "`ollama serve`")

        # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        # í†µê³„
        if "messages" in st.session_state and st.session_state.messages:
            st.markdown("---")
            st.metric("ì´ ëŒ€í™” ìˆ˜", len(st.session_state.messages) // 2)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ìºì‹±ë¨)
    rag_pipeline = initialize_rag_pipeline(
        model_name=model_option,
        temperature=temperature,
        use_reranking=use_reranking
    )

    # RAG ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
    if rag_pipeline is None:
        st.stop()

    # ëŒ€í™” ì´ë ¥ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°)
        with st.chat_message("assistant"):
            try:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
                response_placeholder = st.empty()
                full_response = ""

                for chunk in rag_pipeline.stream_query(prompt):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")

                # ìµœì¢… ì‘ë‹µ í‘œì‹œ (ì»¤ì„œ ì œê±°)
                response_placeholder.markdown(full_response)

                # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response
                })

            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                logger.error(f"ì§ˆì˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)

                st.session_state.messages.append({
                    "role": "assistant",
                    "content": "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
                })

    # í‘¸í„°
    st.markdown("---")
    st.caption(
        f"Â© 2025 í“¨ì³ì‹œìŠ¤í…œ ì±—ë´‡ | "
        f"Powered by ChromaDB + BGE-M3 + FlashRank + Ollama | "
        f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d')}"
    )


if __name__ == "__main__":
    main()
